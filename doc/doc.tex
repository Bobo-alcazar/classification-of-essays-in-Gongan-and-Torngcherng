\documentclass[12pt, a4paper, oneside]{ctexart}
\usepackage{amsmath, graphicx, geometry, hyperref}


% 导言区

\title{基于scikit-learn库的文本分类任务测试}
\author{叶蓁 211506011}
\date{}

\setCJKmainfont{I.Ming-8.00.ttf}
\setCJKsansfont{LXGWClearGothic-Regular.ttf}
\setCJKmonofont{SourceHanMono.ttc}

%\geometry{left=1.5cm, right=1.5cm, top=3cm, bottom=3cm}

\begin{document}

\maketitle

\section{任务简介}

本任务目的是进行一次基于传统机器学习的文本分类任务实践。

现今机器学习已经有大量很成熟的工具，不需要我们从头开始设计。总的来说，有低代码可视化平台和编程工具包两类工具可供我们使用，为了更加自由地处理数据和更加便捷地测试和输出测试结果，我们选择使用编程语言工具包来完成。具体来说，我们选择使用python语言的scikit-learn库来完成，因为scikit-learn库是当前最为常用的机器学习工具包，且提供的算法丰富，可供我们进行大量比较，API结构清晰，便于使用，还提供了便捷的测试结果统计和评估函数。

本任务选取的分类对象是明末公安派与清初桐城派的散文。在文学史上，桐城派很大程度是为反对公安派而产生的，两派散文有较大差异，应当能够被简单的基于机器学习的文本分类模型区分。两派的文章差异体现在用词、句法、篇章结构、思想内涵等多个方面，后两个部分一般来说由篇章分析和主题提取两类任务负责。本任务仅基于用词特点，建立词袋模型（Bag-of-words model）进行训练。

总的流程是，先对两派散文分别选取一定的范围，并在网络上爬取，存储下来。需要注意的是，不同网络资源的结构化程度不同，结构化较强的，已以篇为单位整理好，并施句读；结构化较弱的，可能不仅无句读，且连篇章界限亦无显性标识。对于后者，需要根据文本规律划分篇章。在可能的情况下，应尽量选取结构化程度更高的资料，以避免预处理中不必要的工作。

接下来，对文本进行预处理。具体来说，对于无句读的文档进行断句。尔后对每篇文档进行分词处理，将每篇文档表示为一个字符串序列。在此之后，根据字符串序列建立独热（one-hot）表示或TF-IDF加权表示，将每篇文档表示为一个向量。

将向量化的文档分别以互信息筛选和方差筛选两种方法降维，并以一定比例划分为训练集和测试集后，分别采取支持向量机（SVM）、对数几率回归（logistic regression）、朴素贝叶斯（naive Bayes）、K近邻（K-nearest neighbors）、决策树（decision tree）、随机森林（random forest）六种算法根据训练集进行建模，并应用于测试集。记录下测试集的预测结果与实际结果，计算每种情况的F值与AUC值，分析对于此任务，各算法的优劣。

\section{语料库制作}

任务的第一步是爬取网络资源制作语料库。对于公安派和桐城派，我们分别选取其最具代表性的公安三袁的作品集，袁宏道\href{《袁中郎全集》}{https://ctext.org/wiki.pl?if=gb&res=142162&remap=gb}、袁中道\href{《珂雪斋集》}{https://zh.wikisource.org/wiki/%E7%8F%82%E9%9B%AA%E9%BD%8B%E9%9B%86}、袁宗道\href{《白苏斋类集》}{https://zh.wikisource.org/zh-hant/%E7%99%BD%E8%98%87%E9%BD%8B%E9%A1%9E%E9%9B%86}和桐城三祖的作品集方苞\href{《方望溪先生全集》}{https://zh.wikisource.org/wiki/%E6%96%B9%E6%9C%9B%E6%BA%AA%E5%85%88%E7%94%9F%E5%85%A8%E9%9B%86_(%E5%9B%9B%E9%83%A8%E5%8F%A2%E5%88%8A%E6%9C%AC)}、刘大櫆\href{《海峰文集》}{https://ctext.org/wiki.pl?if=gb&res=110875}、姚鼐\href{《惜抱轩文集》}{https://zh.wikisource.org/wiki/%E6%83%9C%E6%8A%B1%E8%BB%92%E6%96%87%E9%9B%86}，它们分别可以在维基文库和中国哲学书电子化计划两网站上看到，且每部集子都配有带链接的目录页，便于爬取。维基媒体的所有网站都是不设反爬措施的（实际上维基媒体鼓励合规的自由利用网站数据），中国哲学书电子化计划网站设有反扒措施，但并不严密，且主要是针对爬取网站公开书影的，对文本信息页面爬取管得不严，综上，我们不专门设计避免反扒措施的流程，不过伪装浏览器的请求头还是必要的。

此外，本任务仅是对散文流派的文本分类，其他文体不仅在用词上与散文有较大差异，在文学上也不被纳入桐城派、公安派的分类之中。以上作品集有的并非是散文类集性质，其中包含的其他文体因而是需要排除的。还有一些目录、序一类章节，也是需要排出的。这需要排除的内容被手动写进了代码。




%\begin{figure}[htbp]
%	\centering
%	\includegraphics[width=.8\textwidth]{D:/pic.png}
%	\caption{這是標題}
%\end{figure}




\end{document}